## Statement of need

Benchmarking is a standard practice to illustrate the strengths and weaknesses of algorithms regarding different problem characteristics.
In machine learning, benchmarking often involves assessing the performance of the ML models, namely how well they predict labels for new samples (supervised learning) or detect patterns among samples with no pre-existing labels (unsupervised learning) in a group of benchmark datasets [@doi:10.1016/j.neunet.2012.02.016; @doi:10.1145/1143844.1143865].
PMLB provides this suite of datasets as well as the framework for conducting automatic evaluation of the different algorithms.

The first release of PMLB [@doi:10.1186/s13040-017-0154-4] received overwhelmingly positive feedback from the ML community, reflecting the pressing need for a collection of standardized datasets to evaluate models.
As the repository becomes more widely used, community members have requested new features such as additional information about the datasets as well as new functions to select datasets given specific criteria.
In this paper, we reviewed existing functionality and presented new enhancements that help facilitate the user and contributor's frictionless interaction with the repository.
