## Differentiating attributes

### New datasets with rich metadata

Since its initial release, we have made major improvements in the collection of datasets as well as other helpful supporting features.
We have redesigned the repository structure, and the collection now has benchmark datasets for regression problems (Fig. {@fig:home-chart}).
To fulfill several users' [request](https://github.com/EpistasisLab/penn-ml-benchmarks/issues/13), each dataset also has a metadata.yaml file --- an example of can be viewed [here](https://github.com/EpistasisLab/penn-ml-benchmarks/blob/master/datasets/molecular_biology_promoters/metadata.yaml).
As the name suggests, these files contain general information about the datasets themselves.
Specifically, for each dataset, the metadata file includes the link to the original source of the dataset, a general description, the publication associated with the dataset generation, the type of problem (i.e., classification or regression), keywords (e.g., simulation, ecological, bioinformatics), and the features’ description and their coding (e.g., ‘non-promoter’= 0,  ‘promoter’= 1).

![Characteristics of datasets in the PMLB collection](images/pmlb-home-chart.png){#fig:home-chart width="100%"}

We are grateful for the open source contributors who have gradually increased the number of datasets with metadata.
By carefully examining the data source and gather important information about the dataset, contributors have flagged serious issues with some datasets such as the [incorrect column](https://github.com/EpistasisLab/penn-ml-benchmarks/issues/54) assigned as 'target' in the [bupa](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/datasets/bupa) dataset.

### User-friendly interfaces

On the project PMLB [home page](https://epistasislab.github.io/penn-ml-benchmarks/), the user can now browse, sort, filter and search from a lookup table of datasets with summary statistics (Fig. {@fig:home-tab}).
To select datasets with specific parameters, one can type in the box at the bottom of each numeric column in the format `low ... high`.
For example, if the user wants to view all _classification_ datasets with _80 to 100 observations_, they would select _classification_ at the bottom of `Task` and type `80 ... 100` at the bottom of the `n_observations` column.
The `CSV` button enables the download of the table with selected criteria.

![The user can browse, sort, filter and search the summary statistics table](images/pmlb-home-tab.png){#fig:home-tab width="100%"}

On this main website, we also publish a detailed contribution guide with step-by-step instruction on how to get started.
Our goal was to simplify the steps newcomers need to take to contribute.
Specifically, we have automated many of these steps with continuous integration using the GitHub Actions service.
When a new dataset is added, its summary statistics (e.g., number of observations, number of classes, etc.) is automatically computed, its profiling report is generated (see below), a corresponding metadata template is added to the dataset folder, and the list of dataset names is updated.
Other checks included in the continuous integration workflow also help reduce both the reviewer and contributor’s workload.

In addition to the Python library, we have integrated an [R library](https://github.com/EpistasisLab/pmlb) -- both can be simply installed with `pip install pmlb` or `install.packages('pmlb')`, respectively.
This R library has been adapted from a [separate repository](https://github.com/makeyourownmaker/pmlblite) that seemed to be unmaintained.
However, because the original source code was released under a [GPL-2 license](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html), we were able to adapt the code to make it compatible with the new repository structure in this release and additional functionality.
Is detailed vignettes also make PMLB a helpful resource for new users to begin testing their methods with benchmark datasets.
These vignettes contain straightforward examples of how to automate the tedious task of comparing different ML methods on all the benchmark datasets based on specified metrics.

PMLB now includes original data rows with missing data (i.e., NA). 
The core function of PMLB, fetch_data(), retains previous behavior (`dropna=True`) by default, which excludes all rows with missing data. 
However, if the user chooses to treat the missing values differently, they can use fetch_data() with the option `dropna=False` to obtain the original dataset and apply their own removal or imputation method.
Defining the neighborhood to be the datasets' metadata/characteristics space, we also enabled the option to select the nearest PMLB datasets given a data frame.
This functionality would be helpful for users who would like to find PMLB datasets with similar characteristics to their own to make inference on their dataset, e.g., where to start the hyperparameter search.
An API reference that details the user-facing functions and variables within the PMLB python library is included in the PMLB page.

#### Pandas profiling reports 

For each dataset, we use [pandas profiling](https://pandas-profiling.github.io/pandas-profiling/) to provide a report for exploratory analysis.
In addition to descriptive statistics of the features as provided by `pandas.describe` (Python) or `skimr::skim` (R), pandas profiling gives a more extensive exploration of the dataset such as correlation structure among the features and flaggings of duplicate rows.
Browsing a report allows a user or contributor to easily assess the dataset quality and make necessary changes.
For example, if a feature is flagged as having a unique value for each row in the report, it is likely that this feature is an observation identifier and should be removed from the dataset.

The profiling reports can be accessed by clicking on the dataset name in the data table or the data point in the interactive chart on the home page.
Alternatively, all the reports can be viewed on the repository's [gh-pages](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/gh-pages/profile) branch.

### Efficiency

We have significantly reduced the repository size and started to track all data files with [Git Large File Storage](https://git-lfs.github.com/) for efficient cloning of the repository.
With the Large File Storage service, we now store large files on the GitHub.com remote server (with no limits on data storage) and include text pointers to these files in our repository.
Users who want to interact with the entire repository on their local machine only need git LFS [installed and set up for their user account](https://git-lfs.github.com/) or download the zip file from GitHub.
